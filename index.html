<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jie He</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jie He</name>
              </p>
              <p>My name is Jie He. I am currently a second-year Ph.D. student at University of Edinburgh. I am fortunately advised by <a href="http://knowledge-representation.org/j.z.pan/">Prof. Jeff Pan</a>. My research is generously supported by Edinburgh Doctoral College and  <a href="https://machinelearning.apple.com/updates/apple-scholars-aiml-2024/">2024 Apple Schaolars in AI/ML</a> Fellowships.</p>
              </p>
              <p>
               Before I join UoE, I was fortunated to work at <a href="https://tjunlp-lab.github.io/">TJU-NLP</a> since Sept. 2019, advised by <a href="http://cic.tju.edu.cn/faculty/xiongdeyi/">Prof. Deyi Xiong</a> and  advised by <a href="https://liuquncn.github.io/">Prof. Qun Liu</a>. Also, I worked as an intern under the supervision of <a href="https://www.neuralnoise.com/">Prof. Pasquale Minervini</a> in Fall 2021. 
                I obtained my M.S. degree in Computer Science Department at Tianjin University and my bachelor's degree in Software College, Shandong University.
              </p>
              <p>
                <font color="red">I am actively looking for PhD Intern position starting from 2024.</font>
              </p> 
              <p style="text-align:center">
                <a href="mailto:j.he@ed.ac.uk">Email</a> &nbsp/&nbsp
                <a href="data/CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=VMD_HuYAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/Jiehenlp">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/probe2">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/jieh.jpg"><img style="width:90%;max-width:90%" alt="profile photo" src="images/jieh-modified.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My primary research interests lie in natural language processing (NLP), especially in <strong>commonsense reasoning</strong> and <strong>grammactical error correction</strong>. Currently, my research are driven by two goals:</p>
              <p>
                <span class="highlight">Reasoning: </span> Build an AI system that taps into intrinsic knowledge and uses extrinsic knowledge for logical reasoning.</p>
              <p> 
                <span class="highlight">Generalization:</span> Design a creative AI model that can use existing experience to solve new problems. </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/EMNLP2023.jpg" alt="blind-date" width="240" height="150">
            </td>
            <td width="75%" valign="middle">
                <papertitle>Instances and Labels: Hierarchy-aware Joint Supervised Contrastive Learning for Hierarchical Multi-Label Text Classification
</papertitle>
              <br>
              <p>Simon Chi Lok U*, <strong>Jie He*</strong>,  Victor Gutierrez-Basulto and Jeff Z. Pan</p>
              <em>EMNLP 2023 finding</em>
              <a href="https://arxiv.org/abs/2310.05128">[paper]</a> <a href="https://github.com/simonucl/HJCL">[code]</a> 
              <p> Hierarchical multi-label text classification (HMTC) aims at utilizing a label hierarchy in multi-label classification. Recent approaches to HMTC deal with the problem of imposing an over-constrained premise on the output space by using contrastive learning on generated samples in a semi-supervised manner to bring text and label embeddings closer. However, the generation of samples tends to introduce noise as it ignores the correlation between similar samples in the same batch. One solution to this issue is supervised contrastive learning, but it remains an underexplored topic in HMTC due to its complex structured labels. To overcome this challenge, we propose HJCL, a Hierarchy-aware Joint Supervised Contrastive Learning method that bridges the gap between supervised contrastive learning and HMTC. Specifically, we employ both instance-wise and label-wise contrastive learning techniques and carefully construct batches to fulfill the contrastive learning objective. Extensive experiments on four multi-path HMTC datasets demonstrate that HJCL achieves promising results and the effectiveness of Contrastive Learning on HMTC.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/acl2023.jpg" alt="blind-date" width="240" height="200">
            </td>
            <td width="75%" valign="middle">
                <papertitle>BUCA: A Binary Classification Approach to Unsupervised Commonsense Question Answering</papertitle>
              <br>
              <p><strong>Jie He</strong>, Simon Chi Lok U, Victor Gutierrez-Basulto and Jeff Z. Pan</p>
              <em>ACL 2023</em>
              <a href="https://arxiv.org/abs/2305.15932">[paper]</a> <a href="https://github.com/probe2/BUCA">[code]</a> 
              <p>Unsupervised commonsense reasoning (UCR) is becoming increasingly popular as the construction of commonsense reasoning datasets is expensive, and they are inevitably limited in their scope. A popular approach to UCR is to fine-tune language models with external knowledge (e.g., knowledge graphs), but this usually requires a large number of training examples. In this paper, we propose to transform the downstream multiple choice question answering task into a simpler binary classification task by ranking all candidate answers according to their reasonableness. To this end, for training the model, we convert the knowledge graph triples into reasonable and unreasonable texts. Extensive experimental results show the effectiveness of our approach on various multiple choice question answering benchmarks. Furthermore, compared with existing UCR approaches using KGs, ours is less data hungry.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/acl2021.png" alt="blind-date" width="200" height="240">
            </td>
            <td width="75%" valign="middle">
                <papertitle>TGEA: An Error-Annotated Dataset and Benchmark Tasks for Text Generation from Pretrained Language Models</papertitle>
              <br>
              <p><strong>Jie He*</strong>, Bo peng*, Yi Liao, Qun Liu and Deyi Xiong</p>
              <em>ACL 2021</em>
              <a href="https://aclanthology.org/2021.acl-long.469.pdf">[paper]</a> <a href="https://download.mindspore.cn/dataset/TGEA/">[dataset]</a> 
              <p>In order to deeply understand the capability of pretrained language models in text generation and conduct a diagnostic evaluation, we propose TGEA. We use carefully selected prompt words to guide GPT-2 to generate candidate sentences, from which we select 47K for error annotation. We create an error taxonomy to cover 24 types of errors occurring in these erroneous sentences according to the nature of errors with respect to linguistics and knowledge (e.g., commonsense). Each error is hence manually labeled with comprehensive annotations, including the span of the error, the associated span, minimal correction to the error, the type of the error, and rationale behind the error. Furthermore, we use TGEA as a benchmark dataset and propose a series of automatic diagnosis tasks, including error detection, error type classification, associated span detection, error rationale generation, to further promote future study on the automatic error detection and correction on texts generated by pretrained language models.</p>
            </td>
          </tr>

          <br>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/emnlp2020.png" alt="clean-usnob" width="200" height="160">
            </td>
            <td width="75%" valign="middle">

                <papertitle>The Box is in the Pen: Evaluating Commonsense Reasoning in Neural Machine Translation</papertitle>

              <br>
              <p><strong>Jie He*</strong>, Tao Wang*, Deyi Xiong and Qun Liu</p>
              <em>EMNLP 2020 finding</em>
              <a href="https://aclanthology.org/2020.findings-emnlp.327/">[paper]</a> <a href="https://github.com/tjunlp-lab/CommonMT">[dataset]</a>
              <p> In this paper, we present a test suite to evaluate the commonsense reasoning capability of neural machine translation. The test suite consists of three test sets, covering lexical and contextless/contextual syntactic ambiguity that requires commonsense knowledge to resolve. We manually create 1,200 triples, each of which contain a source sentence and two contrastive translations. We conduct extensive experiments on the test suite to evaluate commonsense reasoning in neural machine translation and investigate factors that have impact on this capability. Our experiments and analyses demonstrate that neural machine translation performs poorly on commonsense reasoning.</p>
            </td>
          </tr>

          <br>


        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Personal information</heading>
              <p>I love philosophy. My favorite philosopher is Nietzsche, and I also enjoy reading Kant's Critique of Pure Reason. I hope to explore more studies combining linguistics and philosophy such as Wittgenstein's language games in the future.</p>
              <p><font color="#20B2AA">  I am open to academic collaborations and please drop me an email if you are interested in collaborating with me! </font></p>
            </td>
          </tr>
        </tbody></table>



        <table width="60%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                    <tr>
                        <td style="padding:0px">
                            <br>
                            <br>
                            <div>
<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=an1OxH7o9dvs29tcBaLmZmi9l5VFKNvwS6CWT24y7Sw&cl=ffffff&w=a"></script>                                                             </div>
                        </td>
                    </tr>
                </tbody>
        </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px">
                <p font-size:small;="">
                    <br>
                    <br>
                    </p><div style="float:left;">
                        Updated at Sep. 2022
                    </div>
                    <div style="float:right;">
                        Thanks <a href="https://jonbarron.info">Jon Barron</a> for this amazing template
                    </div>
                    <br>
                    <br>        
                <p></p>                           
            </td>
          </tr>
        </tbody></table>

      </td>
    </tr>
  </table>
</body>

</html>





